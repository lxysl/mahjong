# 麻将 AI 开发流程文档（V1）

基于 `/Users/lxy/PycharmProjects/mahjong/rule.md` 的本地训练版规则，目标是从零自博弈训练模型，并用于真实牌局中的“手动录入 + AI 决策建议”场景。

## 1. 目标与边界

## 1.1 目标

- 构建可复现、可训练的 4 人麻将环境（136 张、赖子、特殊胡型、自定义 reward）。
- 在无历史对局数据前提下，通过自博弈训练得到可用策略模型。
- 提供“非 UI 形态”的交互接口：你输入牌局事件，AI 输出建议动作。

## 1.2 非目标（当前阶段不做）

- 不开发命令行 UI、网页 UI 或桌面可视化。
- 不接入线上对战平台。
- 不做语音/图像识别自动录入（全部由你手动输入）。

## 2. 总体技术方案

采用“三层解耦”：

1. 规则环境层（Rule + Env）：
  - 负责状态机、合法动作、胡牌判定、reward 结算。
2. 训练层（Self-Play RL）：
  - 负责自博弈采样、策略优化、模型评估、模型版本管理。
3. 推理交互层（API）：
  - 负责接收人工录入的牌局事件、维护对局状态、返回 AI 推荐动作。

## 3. 核心设计约定

## 3.1 状态表示（训练与推理统一）

- 私有信息：
  - 自己手牌计数（含赖子数量）。
- 公共信息：
  - 庄家、当前行动玩家、剩余牌墙数、弃牌池、明牌副露、赖子牌、最近动作历史。
- 不可见信息：
  - 其他玩家暗手牌（训练环境内部可见，但对策略网络输入不可见）。

## 3.2 动作空间（统一编码 + 合法掩码）

- 基础动作：
  - `pass`、`hu`、`discard(tile)`、`chi(pattern)`、`peng(tile)`、`ming_gang(tile)`、`an_gang(tile)`、`bu_gang(tile)`。
- 采用固定动作 ID 表 + 动作参数编码。
- 每一步由规则引擎生成 `legal_action_mask`，模型仅在合法动作中决策。

## 3.3 奖励函数

严格按 `rule.md`：

- 点胡 `+1`
- 自摸 `+2`（与点胡互斥）
- 明杠 `+1`
- 暗杠 `+2`
- 杠上开花按“暗杠 + 胡牌”叠加
- 输家每人 `-1`

## 4. 项目结构建议

```text
/Users/lxy/PycharmProjects/mahjong
├── rule.md
├── development_plan.md
├── docs/
│   ├── state_encoding.md
│   ├── action_space.md
│   └── training_metrics.md
├── mahjong_ai/
│   ├── rules/
│   │   ├── tiles.py
│   │   ├── hand_checker.py
│   │   ├── laizi.py
│   │   └── scoring.py
│   ├── env/
│   │   ├── game_state.py
│   │   ├── transition.py
│   │   ├── legal_actions.py
│   │   └── self_play_env.py
│   ├── agent/
│   │   ├── network.py
│   │   ├── policy.py
│   │   └── inference.py
│   ├── training/
│   │   ├── rollout.py
│   │   ├── ppo_trainer.py
│   │   ├── arena.py
│   │   └── checkpoint.py
│   └── serving/
│       ├── session.py
│       ├── event_parser.py
│       └── recommend.py
└── tests/
    ├── test_rules_*.py
    ├── test_env_*.py
    └── test_serving_*.py
```

## 5. 分阶段开发流程与任务

## 阶段 A：规则引擎落地（先保证“算得对”）

### 目标

- 完整实现 `rule.md`，并通过单元测试覆盖关键规则分支。

### 任务

1. 实现牌表示与转换：
  - 统一牌编码（建议 34 种基础牌索引 + 实例计数）。
2. 实现赖子逻辑：
  - 翻牌映射、循环规则、赖子替代判定。
3. 实现胡牌判定：
  - 标准胡（将牌仅 2/5/8 序数牌）；
  - 特殊胡型（七对、十三幺、全不靠、组合龙）。
4. 实现动作合法性：
  - 吃上家、碰任意家、补杠仅自摸升级、杠后杠尾补牌。
5. 实现胡牌冲突裁决：
  - 禁一炮多响，按座位顺序判定。
6. 实现奖励结算：
  - 即时奖励（杠）+ 终局奖励（胡与输家）。

### 验收标准

- `tests/` 中至少 80 个规则用例通过。
- 每条规则在测试中有正反例（能做/不能做）。
- 关键边界（赖子循环、杠上开花、七对加分）有专门用例。

## 阶段 B：训练环境封装（Gym 风格）

### 目标

- 产出可并行采样的自博弈环境，支持大量对局 rollout。

### 任务

1. 定义 `reset()/step()` 接口与 observation schema。
2. 接入 `legal_action_mask`。
3. 支持四座位轮转与庄家标记。
4. 支持随机种子和可复现实验。
5. 记录完整对局日志（可回放调试）。

### 验收标准

- 单线程可稳定模拟 >= 1 万局无异常中断。
- 同种子结果可复现（关键状态一致）。

## 阶段 C：基线策略与对照体系

### 目标

- 在 RL 前先建立可比较基线，避免“训练了但不知道是否变好”。

### 任务

1. 随机策略基线（Random）。
2. 规则启发式基线（Heuristic）：
  - 优先胡牌；
  - 可杠则杠（受风险规则约束）；
  - 弃牌按孤张/危险度简化策略。
3. 对战评估器（Arena）：
  - 模型 vs Random；
  - 模型 vs Heuristic；
  - 自对弈 Elo 估计。

### 验收标准

- 训练前基线指标可稳定复现，形成初版评估报表。

## 阶段 D：自博弈训练（PPO + 动作掩码）

### 目标

- 从零开始训练策略网络，得到可用模型。

### 任务

1. 模型结构：
  - 输入：手牌 + 公共信息 + 历史特征；
  - 输出：策略 logits + 价值估计。
2. 训练算法：
  - PPO（推荐）；
  - action mask 保证仅合法动作参与采样。
3. 自博弈机制：
  - 共享参数四人自博弈；
  - 引入“历史版本池”稳定训练（当前模型与旧模型混打）。
4. 训练监控：
  - 胜率、平均回报、策略熵、非法动作率（应为 0）。
5. checkpoint 管理：
  - 周期保存；
  - 自动挑选最佳模型（按 Arena 指标）。

### 验收标准

- 模型对 Heuristic 的平均回报长期为正。
- 非法动作率持续为 0。
- 训练过程无明显崩溃（指标非随机震荡）。

## 阶段 E：实战录入推理接口（无 UI）

### 目标

- 支持你在真实牌局中手动录入事件，并实时得到 AI 建议。

### 任务

1. 会话管理 `GameSession`：
  - 初始化：座位、庄家、起手牌、赖子指示牌/赖子牌；
  - 维护当前回合状态。
2. 事件输入协议 `apply_event(event)`：
  - 事件类型：摸牌、打牌、吃、碰、明杠、暗杠、补杠、胡、过。
3. 一致性校验：
  - 非法事件拒绝并返回错误原因；
  - 支持 `undo_last_event()` 便于人工录入纠错。
4. 决策输出 `recommend_action()`：
  - 返回 top-k 动作、主推荐、置信度、简要理由（可选）。

### 验收标准

- 能完整回放一局手工输入日志并输出每一步建议。
- 录入错误可回滚，不破坏会话状态。

## 阶段 F：联调与可用性打磨

### 目标

- 把训练模型与实战接口打通，达到“可带去牌桌试用”。

### 任务

1. 端到端联调：输入事件 -> 模型推理 -> 输出建议。
2. 时延优化：单步建议尽量控制在 300ms 内（本机目标）。
3. 稳定性测试：长局、多局连续推理。
4. 失败兜底：
  - 当输入状态不一致时返回“需人工确认”的明确提示，不给误导建议。

### 验收标准

- 连续 200 局模拟输入无状态错乱。
- 推理接口可稳定返回合法建议动作。

## 6. 关键风险与应对

1. 规则复杂导致判定错误：
  - 先写测试再写实现，规则模块与训练模块解耦。
2. 自博弈不稳定：
  - 使用历史模型池 + 定期 Arena 回归。
3. 人工录入易出错：
  - 强一致性校验 + 撤销功能 + 事件日志。
4. 部分可观测带来的策略偏差：
  - 训练输入严格限制为可观测信息，避免训练/实战输入分布不一致。

## 7. 里程碑与交付物

1. M1（规则引擎完成）：
  - 交付：`rules/` + 规则测试集。
2. M2（环境可训练）：
  - 交付：`env/` + rollout 日志回放。
3. M3（首个可用模型）：
  - 交付：checkpoint + Arena 报告。
4. M4（实战接口可用）：
  - 交付：`serving/` API + 手工录入示例脚本。

## 8. 首批待执行任务（按优先级）

1. T001：确定 34 种牌编码规范与字符串协议（如 `1m/9p/E/C`）。
2. T002：实现赖子映射与单元测试。
3. T003：实现标准胡牌判定（含 2/5/8 将限制）与测试。
4. T004：实现四类特殊胡型判定与测试。
5. T005：实现动作合法性生成器（含吃碰杠胡冲突裁决）与测试。
6. T006：实现 reward 结算模块与测试。
7. T007：封装 `SelfPlayEnv` 并完成随机策略跑通。
8. T008：实现 Heuristic 基线与 Arena 评估脚本。
9. T009：接入 PPO 训练循环，产出首个 checkpoint。
10. T010：实现 `GameSession` + `apply_event` + `recommend_action`。

---

如果按此文档推进，建议先完成阶段 A 与 B，再开始训练；否则模型会学习到错误规则并放大偏差。
